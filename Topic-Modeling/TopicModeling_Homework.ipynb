{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"DS_414_Topic_Modeling_Assignment.ipynb","provenance":[{"file_id":"1KTNIuFlkaWNUe1LIvm05h8WMBtd34a-y","timestamp":1630264104030},{"file_id":"1crZPH84tb8O5-k018-39o6qE04eC7dZm","timestamp":1630102535129}],"collapsed_sections":[]},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5utTU63WzGd3"},"source":["# Topic Modeling\n","## *Data Science Unit 4 Sprint 1 Assignment 4*\n","\n","\n","![](https://drive.google.com/uc?export=view&id=1Aze8Xe_ZZpV22IwFNUH09T2howHmF1AK)\n","\n","[Image Credit: slides from Ben Mabey](https://speakerdeck.com/bmabey/visualizing-topic-models)\n","\n","Apply Topic Modeling to Analyze a corpus of Amazon reviews\n","\n","- Load in the Amazon Review dataset\n","- Clean the dataset \n","- Vectorize the dataset \n","- Fit a Gensim LDA topic model on Amazon Reviews\n","- Select appropriate number of topics\n","- Create some dope visualization of the topics\n","- Write a few bullets on your findings in markdown at the end"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X0x7DXmr8zYE","executionInfo":{"status":"ok","timestamp":1633045683267,"user_tz":420,"elapsed":3369,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}},"outputId":"221278f8-7182-4ba9-b2b5-d1be90807add"},"source":["!pip install pyLDAvis==3.3.1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyLDAvis==3.3.1 in /usr/local/lib/python3.7/dist-packages (3.3.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.3.1) (1.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.3.1) (0.22.2.post1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.3.1) (2.11.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.3.1) (57.4.0)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.3.1) (1.3.3)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.3.1) (2.7.3)\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.3.1) (0.0)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.3.1) (1.21.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.3.1) (0.16.0)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.3.1) (3.6.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.3.1) (1.0.1)\n","Requirement already satisfied: funcy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.3.1) (1.16)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis==3.3.1) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis==3.3.1) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis==3.3.1) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis==3.3.1) (5.2.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis==3.3.1) (2.0.1)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQGwb9xbFKIr","executionInfo":{"status":"ok","timestamp":1633045690600,"user_tz":420,"elapsed":3363,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}},"outputId":"2d3e7e21-407d-4df4-848d-72ad80cfe178"},"source":["!pip install pandarallel==1.4.8"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandarallel==1.4.8 in /usr/local/lib/python3.7/dist-packages (1.4.8)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from pandarallel==1.4.8) (0.3.4)\n"]}]},{"cell_type":"code","metadata":{"id":"nUPIrjOB_-BR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633045373118,"user_tz":420,"elapsed":30550,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}},"outputId":"e9986ed7-f2aa-4bc8-aea7-dae1876356dc"},"source":["!python -m spacy download en_core_web_md"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en_core_web_md==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n","\u001b[K     |████████████████████████████████| 96.4 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.62.3)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.21.2)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n","Building wheels for collected packages: en-core-web-md\n","  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-py3-none-any.whl size=98051302 sha256=c30c3e3f05e8b945326570f07f94cf43dbcc53148c4abe9ec4a6bf6a589b4746\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-8fuuejbz/wheels/69/c5/b8/4f1c029d89238734311b3269762ab2ee325a42da2ce8edb997\n","Successfully built en-core-web-md\n","Installing collected packages: en-core-web-md\n","Successfully installed en-core-web-md-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_md')\n"]}]},{"cell_type":"markdown","metadata":{"id":"isyLmOAZqYR0"},"source":["## Restart runtime!"]},{"cell_type":"code","metadata":{"id":"rXrXC0mrzGd5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633045400428,"user_tz":420,"elapsed":2104,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}},"outputId":"7f868b3f-9038-442a-9861-743fc374bea6"},"source":["import re\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","import spacy\n","spacy.util.fix_random_seed(0)\n","\n","import pyLDAvis\n","import pyLDAvis.gensim_models \n","\n","import gensim\n","import gensim.corpora as corpora\n","from gensim.utils import simple_preprocess\n","from gensim.models import CoherenceModel\n","\n","from pandarallel import pandarallel\n","\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n","  from collections import Iterable\n","/usr/local/lib/python3.7/dist-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n","  from collections import Mapping\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:169: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:286: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:858: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True):\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1094: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1120: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, positive=False):\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1349: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1590: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1723: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n","/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  EPS = np.finfo(np.float).eps\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n","scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n","  _deprecated()\n"]}]},{"cell_type":"markdown","metadata":{"id":"x9ph4ZzpzGd6"},"source":["----\n","### Load the Amazon Review corpus \n","This dataset is located in the Sprint 1 Module 1 `/data` directory. \n","\n","If the provided relative path doesn't work for you, then you'll have to provide the file path so pandas can read in the file."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c_L4bKk_4E84","executionInfo":{"status":"ok","timestamp":1633042539341,"user_tz":420,"elapsed":5495,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}},"outputId":"2fd99623-3d8e-4deb-a76c-764e31fbe41c"},"source":["# A \"brute force\" way to get the Amazon reviews dataset; just clone the Sprint 1 repo!\n","# Overkill, but it does the job!\n","!git clone https://github.com/LambdaSchool/DS-Unit-4-Sprint-1-NLP.git"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'DS-Unit-4-Sprint-1-NLP'...\n","remote: Enumerating objects: 1604, done.\u001b[K\n","remote: Counting objects: 100% (73/73), done.\u001b[K\n","remote: Compressing objects: 100% (66/66), done.\u001b[K\n","remote: Total 1604 (delta 32), reused 26 (delta 7), pack-reused 1531\u001b[K\n","Receiving objects: 100% (1604/1604), 58.12 MiB | 21.79 MiB/s, done.\n","Resolving deltas: 100% (201/201), done.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7__zXIF_504B","executionInfo":{"status":"ok","timestamp":1630270078408,"user_tz":420,"elapsed":85499,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}},"outputId":"f6c28e5f-7a2b-4687-9aad-534ebefbc360"},"source":["!unzip '/content/DS-Unit-4-Sprint-1-NLP/module1-text-data/data/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv.zip'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  /content/DS-Unit-4-Sprint-1-NLP/module1-text-data/data/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv.zip\n","replace Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace __MACOSX/._Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ov482n4pzGd6"},"source":["data_path = \"/content/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\"\n","df = pd.read_csv(data_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-yZZEwZNzGd7","colab":{"base_uri":"https://localhost:8080/","height":271},"executionInfo":{"status":"ok","timestamp":1630270167063,"user_tz":420,"elapsed":129,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}},"outputId":"8e9b4d24-867c-4043-9f78-4df1548748be"},"source":["df.head(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>dateAdded</th>\n","      <th>dateUpdated</th>\n","      <th>name</th>\n","      <th>asins</th>\n","      <th>brand</th>\n","      <th>categories</th>\n","      <th>primaryCategories</th>\n","      <th>imageURLs</th>\n","      <th>keys</th>\n","      <th>manufacturer</th>\n","      <th>manufacturerNumber</th>\n","      <th>reviews.date</th>\n","      <th>reviews.dateSeen</th>\n","      <th>reviews.didPurchase</th>\n","      <th>reviews.doRecommend</th>\n","      <th>reviews.id</th>\n","      <th>reviews.numHelpful</th>\n","      <th>reviews.rating</th>\n","      <th>reviews.sourceURLs</th>\n","      <th>reviews.text</th>\n","      <th>reviews.title</th>\n","      <th>reviews.username</th>\n","      <th>sourceURLs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>AVpgNzjwLJeJML43Kpxn</td>\n","      <td>2015-10-30T08:59:32Z</td>\n","      <td>2019-04-25T09:08:16Z</td>\n","      <td>AmazonBasics AAA Performance Alkaline Batterie...</td>\n","      <td>B00QWO9P0O,B00LH3DMUO</td>\n","      <td>Amazonbasics</td>\n","      <td>AA,AAA,Health,Electronics,Health &amp; Household,C...</td>\n","      <td>Health &amp; Beauty</td>\n","      <td>https://images-na.ssl-images-amazon.com/images...</td>\n","      <td>amazonbasics/hl002619,amazonbasicsaaaperforman...</td>\n","      <td>AmazonBasics</td>\n","      <td>HL-002619</td>\n","      <td>2017-03-02T00:00:00.000Z</td>\n","      <td>2017-08-28T00:00:00Z</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>https://www.amazon.com/product-reviews/B00QWO9...</td>\n","      <td>I order 3 of them and one of the item is bad q...</td>\n","      <td>... 3 of them and one of the item is bad quali...</td>\n","      <td>Byger yang</td>\n","      <td>https://www.barcodable.com/upc/841710106442,ht...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>AVpgNzjwLJeJML43Kpxn</td>\n","      <td>2015-10-30T08:59:32Z</td>\n","      <td>2019-04-25T09:08:16Z</td>\n","      <td>AmazonBasics AAA Performance Alkaline Batterie...</td>\n","      <td>B00QWO9P0O,B00LH3DMUO</td>\n","      <td>Amazonbasics</td>\n","      <td>AA,AAA,Health,Electronics,Health &amp; Household,C...</td>\n","      <td>Health &amp; Beauty</td>\n","      <td>https://images-na.ssl-images-amazon.com/images...</td>\n","      <td>amazonbasics/hl002619,amazonbasicsaaaperforman...</td>\n","      <td>AmazonBasics</td>\n","      <td>HL-002619</td>\n","      <td>2016-08-31T00:00:00.000Z</td>\n","      <td>2017-08-28T00:00:00Z</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>4</td>\n","      <td>https://www.amazon.com/product-reviews/B00QWO9...</td>\n","      <td>Bulk is always the less expensive way to go fo...</td>\n","      <td>... always the less expensive way to go for pr...</td>\n","      <td>ByMG</td>\n","      <td>https://www.barcodable.com/upc/841710106442,ht...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     id  ...                                         sourceURLs\n","0  AVpgNzjwLJeJML43Kpxn  ...  https://www.barcodable.com/upc/841710106442,ht...\n","1  AVpgNzjwLJeJML43Kpxn  ...  https://www.barcodable.com/upc/841710106442,ht...\n","\n","[2 rows x 24 columns]"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"kdwr6x8PzGd7"},"source":["----\n","\n","### Clean data\n","\n","- Create a function called `clean_data` that uses regex expressions to clean your data in preparation for the vectorizer. \n","\n","- Save the clean text data to a column in your dataframe named `clean_text`\n","\n","- Feel free to re-use old code that you have written in previous modules  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g5cL1Amq8iT_","executionInfo":{"status":"ok","timestamp":1630270170899,"user_tz":420,"elapsed":123,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}},"outputId":"5e1e4f93-dccc-4f41-fea4-03c0bd421965"},"source":["df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(28332, 24)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"dgW6kglx8T_l","executionInfo":{"status":"ok","timestamp":1630270172642,"user_tz":420,"elapsed":132,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}},"outputId":"cd6a2c7d-62e6-46c6-b999-57c07438db84"},"source":["df['reviews.text'][8505]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Have had zero issues with longevity or voltage. As good as the major brand batteries'"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"a2513c0ebd04cf8645fe0a1feb1e1a36","grade":false,"grade_id":"cell-fa0950cfe5ef7725","locked":false,"schema_version":3,"solution":true,"task":false},"id":"4xo_hkHKzGd7"},"source":["def clean_data(text):\n","  \"\"\"\n","  Cleans data to remove unwanted characters and punctuation.\n","  \"\"\"\n"," # YOUR CODE HERE\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"a6ceefe5476e71efcb3d36c3e203616f","grade":false,"grade_id":"cell-5b8e2bc0f9a745f3","locked":false,"schema_version":3,"solution":true,"task":false},"id":"H693L3fJzGd8"},"source":["# create a clean_text column by applying  clean_data to your text\n","df['clean_text'] = "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5FMvAhkBzGd8"},"source":["alphebetical_chars = [\"ABCDEFGHIJKLMNOP\"]\n","# check if any of these alphabetical chars exist in your clean chars\n","assert df.clean_text.isin(alphebetical_chars).sum() == 0, \"Did you case normalize your text inside of your clean_data function?\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kzqt1ainzGd8"},"source":["------\n","\n","## Determine number of topics\n","\n","We are going to run an experiment to determine how many topics exists within the `primaryCategories` of `Electronics`. This is the largest primary category containing nearly 14K documents, so we should have plenty of data. \n","\n","Just as we did in the guided project, we'll be running a gridseach over the number of topics and scoring each model using the Coherence metric to determine which number of topics we should use. \n"]},{"cell_type":"code","metadata":{"id":"3jrHewGbzGd9"},"source":["# create a mask for docs that are in the Electronics primaryCategories - save result to `electronics_mask`\n","electronics_mask = df.primaryCategories.isin([\"Electronics\"])\n","\n","# use mask to select all the documents in the Electronics primaryCategories - save result to `df_electronics`\n","df_electronics = df[electronics_mask]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":372},"id":"peb18zW3zGd9","executionInfo":{"status":"ok","timestamp":1630270183672,"user_tz":420,"elapsed":210,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}},"outputId":"a3978c32-5d74-4361-e642-76dc927f65e9"},"source":["df_electronics.head(3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>dateAdded</th>\n","      <th>dateUpdated</th>\n","      <th>name</th>\n","      <th>asins</th>\n","      <th>brand</th>\n","      <th>categories</th>\n","      <th>primaryCategories</th>\n","      <th>imageURLs</th>\n","      <th>keys</th>\n","      <th>manufacturer</th>\n","      <th>manufacturerNumber</th>\n","      <th>reviews.date</th>\n","      <th>reviews.dateSeen</th>\n","      <th>reviews.didPurchase</th>\n","      <th>reviews.doRecommend</th>\n","      <th>reviews.id</th>\n","      <th>reviews.numHelpful</th>\n","      <th>reviews.rating</th>\n","      <th>reviews.sourceURLs</th>\n","      <th>reviews.text</th>\n","      <th>reviews.title</th>\n","      <th>reviews.username</th>\n","      <th>sourceURLs</th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>8343</th>\n","      <td>AVpe7nGV1cnluZ0-aG2o</td>\n","      <td>2014-10-28T11:14:38Z</td>\n","      <td>2019-04-25T09:05:28Z</td>\n","      <td>AmazonBasics Nylon CD/DVD Binder (400 Capacity)</td>\n","      <td>B00DIHVMEA,B00EZ1ZTV0</td>\n","      <td>Amazonbasics</td>\n","      <td>Audio &amp; Video Accessories,TV, Video &amp; Home Aud...</td>\n","      <td>Electronics</td>\n","      <td>http://ecx.images-amazon.com/images/I/41jQha7Z...</td>\n","      <td>amazonbasicsnyloncddvdbinder400capacity/b00ez1...</td>\n","      <td>AmazonBasics</td>\n","      <td>YBB12400R2</td>\n","      <td>2017-11-12T00:00:00.000Z</td>\n","      <td>2019-03-25T00:00:00Z</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>https://www.ebay.com/itm/Amazonbasics-Nylon-Cd...</td>\n","      <td>Great case to keep everything in its place! My...</td>\n","      <td>Excellent product</td>\n","      <td>qs341_5</td>\n","      <td>https://www.ebay.com/itm/AmazonBasics-Nylon-CD...</td>\n","      <td>great case to keep everything in its place my ...</td>\n","    </tr>\n","    <tr>\n","      <th>8344</th>\n","      <td>AVpe7nGV1cnluZ0-aG2o</td>\n","      <td>2014-10-28T11:14:38Z</td>\n","      <td>2019-04-25T09:05:28Z</td>\n","      <td>AmazonBasics Nylon CD/DVD Binder (400 Capacity)</td>\n","      <td>B00DIHVMEA,B00EZ1ZTV0</td>\n","      <td>Amazonbasics</td>\n","      <td>Audio &amp; Video Accessories,TV, Video &amp; Home Aud...</td>\n","      <td>Electronics</td>\n","      <td>http://ecx.images-amazon.com/images/I/41jQha7Z...</td>\n","      <td>amazonbasicsnyloncddvdbinder400capacity/b00ez1...</td>\n","      <td>AmazonBasics</td>\n","      <td>YBB12400R2</td>\n","      <td>2014-06-14T05:00:00Z</td>\n","      <td>2014-08-28T00:00:00Z</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>http://www.amazon.co.uk/gp/product-reviews/B00...</td>\n","      <td>After discarding and getting rid of broken cd ...</td>\n","      <td>It was a much needed storage</td>\n","      <td>Diablita</td>\n","      <td>https://www.ebay.com/itm/AmazonBasics-Nylon-CD...</td>\n","      <td>after discarding and getting rid of broken cd ...</td>\n","    </tr>\n","    <tr>\n","      <th>8345</th>\n","      <td>AVpe7nGV1cnluZ0-aG2o</td>\n","      <td>2014-10-28T11:14:38Z</td>\n","      <td>2019-04-25T09:05:28Z</td>\n","      <td>AmazonBasics Nylon CD/DVD Binder (400 Capacity)</td>\n","      <td>B00DIHVMEA,B00EZ1ZTV0</td>\n","      <td>Amazonbasics</td>\n","      <td>Audio &amp; Video Accessories,TV, Video &amp; Home Aud...</td>\n","      <td>Electronics</td>\n","      <td>http://ecx.images-amazon.com/images/I/41jQha7Z...</td>\n","      <td>amazonbasicsnyloncddvdbinder400capacity/b00ez1...</td>\n","      <td>AmazonBasics</td>\n","      <td>YBB12400R2</td>\n","      <td>2019-02-15T00:00:00.000Z</td>\n","      <td>2019-03-25T00:00:00Z</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>5</td>\n","      <td>https://www.ebay.com/itm/Amazonbasics-Nylon-Cd...</td>\n","      <td>A few dollars more, but I am boycotting amazon</td>\n","      <td>it was worth it</td>\n","      <td>coldbloodblazing</td>\n","      <td>https://www.ebay.com/itm/AmazonBasics-Nylon-CD...</td>\n","      <td>a few dollars more but i am boycotting amazon</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                        id  ...                                         clean_text\n","8343  AVpe7nGV1cnluZ0-aG2o  ...  great case to keep everything in its place my ...\n","8344  AVpe7nGV1cnluZ0-aG2o  ...  after discarding and getting rid of broken cd ...\n","8345  AVpe7nGV1cnluZ0-aG2o  ...      a few dollars more but i am boycotting amazon\n","\n","[3 rows x 25 columns]"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"FfZWF0I9zGd-"},"source":["------\n","### Tokenize your documents \n","\n","Remember that you'll need to use the [**corpora**](https://radimrehurek.com/gensim/corpora/dictionary.html) class from the Gensim library. So definitely check out the docs to learn more about this tool. There is an example on how to do this in the guided project.\n","\n","But before we can use the [**corpora**](https://radimrehurek.com/gensim/corpora/dictionary.html) class, we must first tokenize our articles. \n","\n"]},{"cell_type":"code","metadata":{"id":"2rAMfVfB1PS-"},"source":["# identify how many processors your machine has - save the result to `n_processors`\n","\n","# subtract 1 from n_processors - save the result to `nb_workers`\n","\n","# initialize just like we did in the guided project\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t6pNnk3vzGd-"},"source":["# load in the spaCy language model\n","nlp = spacy.load(\"en_core_web_md\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"05x-KJPIBQa2"},"source":["%%time\n","# create our tokens in the form of lemmas \n","df_electronics['lemmas'] = df_electronics['clean_text'].parallel_apply(lambda x: [token.lemma_ for token in nlp(x) if (token.is_stop != True) and (token.is_punct != True)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WS7vTVqnzGd_"},"source":["### Use the corpora class to prep your data for LDA\n","\n","You'll need to create the same `id2word` and `corpus` objects that we created in the guided projects. So be sure to reference the guided project notebook if you need to. "]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"53bf811aebb39e7ed6fc592851f82f8b","grade":false,"grade_id":"cell-9d92f28649aa999e","locked":false,"schema_version":3,"solution":true,"task":false},"id":"7paL9zS7zGd_"},"source":["# Create lemma dictionary using Dictionary - save result to `id2word`\n","id2word = \n","\n","# Create Term Document Frequency list - save result to `corpus`\n","corpus = \n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vtmHceiazGd_"},"source":["## Gridsearch the number of topics \n","\n","Just as we did in the guided project, we're going to run a `for` loop over a range of possible number of topics and then plot the `coherence_values` to determine which number of topics leads to the most sensible grouping of documents. "]},{"cell_type":"code","metadata":{"id":"FBICFYD4zGeA"},"source":["def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n","    \"\"\"\n","    Compute c_v coherence for various number of topics\n","\n","    Parameters:\n","    ----------\n","    dictionary : Gensim dictionary\n","    corpus : Gensim corpus\n","    texts : List of input texts\n","    limit : Max num of topics\n","\n","    Returns:\n","    -------\n","    model_list : List of LDA topic models\n","    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n","    \"\"\"\n","    coherence_values = []\n","    model_list = []\n","    for num_topics in range(start, limit, step):\n","        model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n","                                                        id2word=id2word,\n","                                                        num_topics=num_topics, \n","                                                        chunksize=100,\n","                                                        passes=10,\n","                                                        random_state=1234,\n","                                                        per_word_topics=True,\n","                                                        workers=2)\n","        model_list.append(model)\n","        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n","        coherence_values.append(coherencemodel.get_coherence())\n","\n","    return model_list, coherence_values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nn20KdDl2HUR"},"source":["%%time\n","model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=df_electronics['lemmas'], start=2, limit=16, step=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k2kJsVLt2P5-"},"source":["start=2; limit=16;  step=2;\n","x = range(start, limit, step)\n","\n","plt.figure(figsize=(20,5))\n","plt.grid()\n","plt.title(\"Coherence Score vs. Number of Topics\")\n","plt.xticks(x)\n","plt.plot(x, coherence_values, \"-o\")\n","\n","plt.xlabel(\"Num Topics\")\n","plt.ylabel(\"Coherence score\")\n","\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"45f72ff929ab8b765d0f0b5f35137f0d","grade":false,"grade_id":"cell-e97661faebf1ac3c","locked":false,"schema_version":3,"solution":true,"task":false},"id":"1Qx7OVX-zGeB"},"source":["# use np.argmax() to get index of largest coherence value from coherence_values - save result to `max_cohereance_val_index`\n","\n","# use `max_coherence_val_index` to index model_list for the corresponding model - save result to `lda_trained_model`\n","\n","max_coherence_val_index = \n","\n","lda_trained_model = \n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7y-wokYcIg9_","executionInfo":{"status":"ok","timestamp":1630271190181,"user_tz":420,"elapsed":244,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}},"outputId":"b5fabbe1-26f3-4770-a011-f892bc76a3c5"},"source":["lda_trained_model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<gensim.models.ldamulticore.LdaMulticore at 0x7f8706e798d0>"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"LwGY4o8fzGeB"},"source":["## Use pyLDAvis to visualize your topics \n","\n","Take a look at the topic bubbles and bar chart for the terms on the right hand side.  \n","\n","- Describe the topic bubbles. \n","- Do they overlap or not? \n","- What does it mean when they overlap? \n","- What does it mean when they don't overlap?\n","- Are the terms in each topic distinct from the topics in the other topic bubbles?\n"]},{"cell_type":"code","metadata":{"id":"9poWZ8WgItYS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630271196635,"user_tz":420,"elapsed":3184,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"}},"outputId":"26627697-0e17-468f-e3c8-71e543e2d5f7"},"source":["!pip install ipywidgets"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (7.6.3)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.0.5)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (4.10.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (1.0.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.1.3)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.5.0)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (3.5.1)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.5)\n","Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.1.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (57.4.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.8.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (1.0.18)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (2.6.1)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (2.6.0)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.7.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (0.2.5)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets) (5.3.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (5.6.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.11.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.3)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (22.2.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.3)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.0.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YBqK345s2oYC"},"source":["# plot your topics here -- using pyLDAvis\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9gie-6YrzGeB"},"source":["## Create a Topic id/name dictionary \n","\n","When populating your topic id/name dictionary, use the index ordering as shown in the viz tool. \n","\n","We'll use a function to map the the viz tool index ordering with the train LDA model ordering. "]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"fc10e9728e5dcf06baf0800c3cdb88ce","grade":false,"grade_id":"cell-4905c0c1050f0d03","locked":false,"schema_version":3,"solution":true,"task":false},"id":"88vMiVwQzGeC"},"source":["# create a dictionary \n","# keys - use topic ids from pyLDAvis visualization \n","# values - topic names that you create \n","# save dictionary to `vis_topic_name_dict`\n","\n","vis_topic_name_dict = {}\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iWLsK27ZzGeC"},"source":["def get_topic_id_lookup_dict(vis, vis_topic_name_dict):\n","    \"\"\"\n","    Both the starting index and the ordering of topic ids bewteen the trained LDA model \n","    and the viz tool are different. So we need to create a look up dictionary that maps \n","    the correct association between topic ids from both sources. \n","    \"\"\"\n","    # value is order of topic ids accoridng to pyLDAvis tool \n","    # key is order of topic ids according to lda model\n","    model_vis_tool_topic_id_lookup = vis.topic_coordinates.topics.to_dict()\n","\n","    # invert dictionary so that \n","    # key is order of topic ids accoridng to pyLDAvis tool \n","    # value is order of topic ids according to lda model\n","    topic_id_lookup =  {v:k for k, v in model_vis_tool_topic_id_lookup.items()}\n","    \n","    return {v:vis_topic_name_dict[k]  for k, v in topic_id_lookup.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tR8xm8IG4M0M"},"source":["Create a topic id/name look up dict \n","that is aligned with the index ordering of the trained LDA model"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"55f23aa9324a225091037f0c5daf2192","grade":false,"grade_id":"cell-d38acb7b250b4079","locked":false,"schema_version":3,"solution":true,"task":false},"id":"dcsa0f_tzGeC"},"source":["topic_name_dict = get_topic_id_lookup_dict(vis, vis_topic_name_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tbeEkAlC4WNi"},"source":["topic_name_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t4NLAlB9zGeC"},"source":["## Use the LDA Model to Assign Each Document a Topic Name\n","\n","Now that we have a topic id/name look up dict that is aligned with the index ordering of the trained LDA model, we can move forward to giving each topic a topic name. \n","\n","The function below has been given to you. However, you highly encouraged to read through it and make sure that you understand what it is doing each step of the way. In fact, a good way to do this is to copy and paste the code inside of the function into a new cell, comment out all the lines of code and line by line, uncomment the code and see the output. "]},{"cell_type":"code","metadata":{"id":"23No1SMKzGeC"},"source":["def get_topic_ids_for_docs(lda_model, corpus):\n","    \n","    \"\"\"\n","    Passes a Bag-of-Words vector into a trained LDA model in order to get the topic id of that document. \n","    \n","    Parameters\n","    ----------\n","    lda_model: Gensim object\n","        Must be a trained model \n","        \n","    corpus: nested lists of tuples, \n","        i.e. [[(),(), ..., ()], [(),(), ..., ()], ..., [(),(), ..., ()]]\n","        \n","    Returns\n","    -------\n","    topic_id_list: list\n","        Contains topic ids for all document vectors in corpus \n","    \"\"\"\n","    \n","    # store topic ids for each document\n","    doc_topic_ids = []\n","\n","    # iterature through the bow vectors for each doc\n","    for doc_bow in corpus:\n","        \n","        # store the topic ids for the doc\n","        topic_ids = []\n","        # store the topic probabilities for the doc\n","        topic_probs = []\n","\n","        # list of tuples\n","        # each tuple has a topic id and the prob that the doc belongs to that topic \n","        topic_id_prob_tuples = lda_trained_model.get_document_topics(doc_bow)\n","        \n","        # iterate through the topic id/prob pairs \n","        for topic_id_prob in topic_id_prob_tuples:\n","            \n","            # index for topic id\n","            topic_id = topic_id_prob[0]\n","            # index for prob that doc belongs that the corresponding topic\n","            topic_prob = topic_id_prob[1]\n","\n","            # store all topic ids for doc\n","            topic_ids.append(topic_id)\n","            # store all topic probs for doc\n","            topic_probs.append(topic_prob)\n","\n","        # get the index for the topic that had the highest probability, for the current document \n","        max_topic_prob_ind = np.argmax(topic_probs)\n","        # get the corresponding topic id\n","        max_prob_topic_id = topic_ids[max_topic_prob_ind]\n","        # store the most probable topic id for the current document\n","        doc_topic_ids.append(max_prob_topic_id)\n","        \n","    return doc_topic_ids"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"09f2b8ebaf696dc05187cecce8c9e251","grade":false,"grade_id":"cell-e28e8774a79ac647","locked":false,"schema_version":3,"solution":true,"task":false},"id":"gI94W2MvzGeD"},"source":["# use get_topic_ids_for_docs to get the topic id for each doc in the corpus - save result to `doc_topic_ids`\n","doc_topic_ids = \n","\n","# create a new feature in df_electronics called topic_id using `doc_topic_ids`\n","df_electronics['topic_id'] = \n","\n","# iterate through topic_id and use the lookup dict `topic_name_dict` to assign each document a topic name\n","# save results to a new feature in df_electronics called `new_topic_name`\n","df_electronics['new_topic_name'] = df_electronics['topic_id'].apply(?)\n","\n","# YOUR CODE HERE\n","raise NotImplementedError()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZa4oKuIzGeD"},"source":["## Congratulations! You have created new topic names for your documents. "]},{"cell_type":"code","metadata":{"id":"kDVAZTa23xkF"},"source":["cols = [\"reviews.text\", \"new_topic_name\", \"topic_id\"]\n","df_electronics[cols].head(15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gkYnBihNzGeE"},"source":["-----\n","# Stretch Goals -- see if you can create a model to classify the reviews into the latent topics you've discovered!\n","\n","\n","- Treat `topic_id` as the `y` (target) vector and train a supervised learning model to predict the topic of each document\n","- Report your results on the Slack channel!"]},{"cell_type":"markdown","metadata":{"id":"eqY0KKxKQxbm"},"source":["## Topic Modeling References\n"]},{"cell_type":"markdown","metadata":{"id":"iKhe7u4IXr0y"},"source":["\n","- [Topic Modeling with Gensim](https://www.tutorialspoint.com/gensim/gensim_topic_modeling.htm) -- Brief Tutorial\n","\n","- [Gensim documentation](https://radimrehurek.com/gensim/index.html) -- look up gensim commands\n","\n","- [`pyLDAvis` documentation](https://pyldavis.readthedocs.io/en/latest/readme.html) -- package for visualizing LDA models\n","\n","- [Visualizing Topic Models](https://speakerdeck.com/bmabey/visualizing-topic-models) -- slides from Ben Mabey\n","\n","- [Exploring the Space of Topic Coherence Measures](https://dl.acm.org/doi/10.1145/2684822.2685324) -- Study of Topic Coherence measures\n","\n","- [Exploring Topic Coherence over many models and many topics](https://www.researchgate.net/publication/232242203_Exploring_Topic_Coherence_over_many_models_and_many_topics) -- Study of automated topic coherence measures"]}]}
